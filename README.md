# **VIVA ΛA VIΔA** 
# $${\color{green}FUTURE \space ENGINEERS \space 2025}$$ 
> ## HELLENIC TEAM

![viva la vida gif](https://github.com/user-attachments/assets/36726f9d-255d-4eb8-8d0d-65e10dfdeb90)


## <ins>*Find us on:*<ins/>
[![Website](https://github.com/user-attachments/assets/fa1ad200-2d47-4041-9703-a8c66c8fa962)](https://www.viva-la-vida.gr/)
[![Youtube](https://github.com/user-attachments/assets/7d082bd6-6470-4c37-881f-5a0bc8695a7f)](https://www.youtube.com/@VIVALAVIDAFutureEnginners)
[![Facebook](https://github.com/user-attachments/assets/b5805394-0b60-4032-b5c8-41c79e29a313)](https://www.facebook.com/groups/1252558666360750)
[![Instagram](https://github.com/user-attachments/assets/1af99f41-1ca3-42ba-8c58-24f4023c03de)](https://www.instagram.com/viva_la_vida_gr/)
[![TikTok](https://github.com/user-attachments/assets/efb356f7-1a95-40ec-9479-392f2e362c01)](https://www.tiktok.com/@viva_la_vida_gr)

## TEAM MEMBERS:
### Nikol Vasilopoulou (user: Quart0xe)
### Panagiotis Mourmouris (user: Panos1431)
### Thanos Karatsis (user: thkaratsis)

*Disclaimer: we did not use these accounts in the creation of this repository. Instead we created a new email and github account specificaly for our team, which we all have access to and utilise across all platforms. 

# INDEX
### *Use the index for an easy navigation of our github or, continue bellow to view our creation process, as well as an overview on everything about the robot.*

## 1. Mobility management
####        1.1 [Motor](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Vehicle/Compartments/Electronics-DC-Motor.md)
####        1.2 [Ackerman](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Vehicle/Compartments/Ackerman%20Steering.md)
####        1.3 [Servo](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Vehicle/Compartments/Electronics-Servo-Motor.md)
####        1.4 [Servo and motor 3d Bases]https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/3d%20designs/3d%20schematics

## 2. Power Management
####        2.1 [Batteries](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Vehicle/Compartments/Batteries.md)
####        2.2 [PCB circuit designs and schematics](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Schematics/PCB)

## 3. Sensors
####        2.1 Distance sensors
####        2.2 Accelerometre / Gyro
####        2.3 Buttons
####        2.4 Camera
####        2.5 Colour sensor
####        2.6 3d sensor bases

## 4. 3D Prints
####        4.1 [3D schematics](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/3d%20designs/3d%20schematics)
####        4.2 [Photos of 3D printed parts](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/3d%20designs/3d%20printed%20part%20photos)

## 5. Pictures 
####        5.1 [Team photos](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Team%20photos)
####        5.2 [Vehicle photographs](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Robot%20Photos)

## 6. Performance videos
####        6.1 [Youtube channel link](https://www.youtube.com/@VIVALAVIDAFutureEnginners)
####        6.2 [All video links](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Videos)

## 7. Daily entries
####        7.1 [Logs](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Training%20meetings%20log)

## 8. Code files
####        8.1 [Open Challenge](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Code%20files/1.%20Open%20Challenge)
####        8.2 [Obstacle Challenge](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Code%20files/2.%20Obstacle%20challenge)
####        8.3 [Colour Calibration](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Code%20files/Python%20files/New%20files%20September%2B/Colour_picker.py)

## 9. Manuals
####        9.1 [Instruction manuals](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Instruction%20Manuals)
####        9.2 [Lego ackerman instructions](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Instruction%20Manuals/Lego%20ackerman%20instructions.pdf)
####        9.3 [Lego differential instructions](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Instruction%20Manuals/Lego%20differential%20instructions.pdf)
####        9.4 [Build details](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Robot%20Photos/Descriptive%20photos)
####        9.5 [Full instructions](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Instruction%20Manuals/Full%20robot%20instructions)

## 10. Extra material
####        10.1 [Team logo](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Team%20-%20Extras/Logo)
####        10.2 [Team name - VIVA ΛΑ VIΔΑ](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Team%20-%20Extras/Name)
####        10.3 [Team shirts](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Team%20-%20Extras/Shirts)


    


# RESEARCH / EXPERIMENTAL PHASE
[Original module photos](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/tree/main/Robot%20Photos/Original%20module)

### Creating [red/green colour detection](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Code%20files/Python%20files/R_G_cannyoutlineWORKING.py)
Using python and opencv we created a program that detects red and green objects and outputs what the computer sees in the prossess.
To do this, firstly we used colour dropping to <ins> find approximate HSV values  of obstacles and edit the ranges in our code </ins>

### $${\color{red}Red:}$$

![image](https://github.com/user-attachments/assets/e6422d6a-328f-4d72-94b2-b9dd87e88be7)

### $${\color{green}Green:}$$

![image](https://github.com/user-attachments/assets/ffb01507-7a3f-49a9-9f3a-950e10f9cfc4)

### [Colour picker used](https://pinetools.com/image-color-picker)

Using the data, we created a range where an object is detected as red or green. The full range that can be detected is min:0 , max:255
```python
    red_lower = np.array([0, 120, 70])
    red_upper = np.array([10, 255, 255])
```
```python
    green_lower = np.array([45, 160, 0])
    green_upper = np.array([100, 255, 150])
```
Then, we applied a HSV mask to separate the objects of each colour from the rest of the feed and used imgcanny to find the corners of the objects detected, using that to add an outline according to their colour (red/green)
Mask:
```python
mask_red1 = cv2.inRange(imgHSV, red_lower, red_upper)
```
Outline:
```python
    img_contours = img.copy()
    for contour in contours_red:
        x, y, w, h = cv2.boundingRect(contour)
        cv2.rectangle(img_contours, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Red rectangle
```
Same for green.
> ### Green:

![image](https://github.com/user-attachments/assets/2dd7592b-8040-411a-b24d-54446ca6187f)
> ### Red:

![image](https://github.com/user-attachments/assets/cb52d939-d804-4d5f-a009-c82977a3d9f6)

Notice how it even detects the pile of red objects on the floor **~3m away**, We want to turn based on the closest object to the robot at any given time.
To detect that, we add limits to the size we consider an object 
> We do this so that it doesnt start turning too early or if it sees a spec of green/red in the background), its basically a filtering process that cuts down on most of the >noise.

For trials we set the minimum requirement in size (area) for a detected colour mass to be considered an object, as **500 pixels**
```python
def get_largest_contour(mask, min_area=500)
```

Next, we made it choose the largest object visible to focus on first
```python
def get_largest_contour(mask, min_area=500):
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return None
    largest = max(contours, key=cv2.contourArea)
    if cv2.contourArea(largest) < min_area: return None
    x, y, w, h = cv2.boundingRect(largest)
    return largest, (x, y), (x + w, y + h), w*h
```

### Updating our colour picking method
The issue with this code was that it needed calibrating in different lighting conditions, so we made out own [colour picker](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Code%20files/Python%20files/New%20files%20September%2B/Colour_picker.py) that worked more accurately with the hsv values our camera detected. With this the obstacle detection became much more time efficient but also accurate.

Colour picker sample
```python
def mouse_callback(event, x, y, flags, param):
    global frame
    if event == cv2.EVENT_LBUTTONDOWN and frame is not None:
        bgr = frame[y, x]
        hsv = cv2.cvtColor(np.uint8([[bgr]]), cv2.COLOR_BGR2HSV)[0][0]
        print(f"Clicked at ({x},{y}) | BGR: {bgr} | HSV: {hsv}")

cv2.namedWindow("Live Feed")
cv2.setMouseCallback("Live Feed", mouse_callback)
```
We use input from our camera to our computer as a live feed, clicking on any pixel to get the HSV values the camera detects straight to our terminal.
We can then imput them straight into our colour detection limits, (red_lower, red_upper, green_lower, green_upper)

### Using what we detected to avoid the obstacles
Now that we can succesfully detect the red and green obstacles, we decided to implement a simple system for the robot to use in order to turn. 
It uses the Objects detected as a reference point and tries to rotate away from them, untill they are outside of the visible area (in other words, when they are far away enough for the robot to not collide with them).
Reminder, the robot must move **right** if it detects **red** and on the flip side, **left** when it detects **green**
> In other words, if we draw a line on the right side of the "turn right" block (red) and the left side of the "turn left" block (green), by moving toward the side it is placed on and effectively moving the entire obstacle out of our view, we manage to pass it.

So for **Green** (Turn left):

<img width="576" height="360" alt="green pillar" src="https://github.com/user-attachments/assets/0db5fb66-a389-43f2-9f20-6747467d4814" />

And for **Right** (Turn right):

<img width="576" height="360" alt="red pillar" src="https://github.com/user-attachments/assets/67523d32-0b6a-445d-900d-beb8b43237e5" />



### Next, we created the [Middle_Lane_Canny](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Code%20files/Python%20files/Middle_Lane_Canny_WORKING.py)
We applied the same code from the previous program, this time switching green and red with white and black in order to diffrentiate the (white) floor from the (black) walls.
From this process, instead of outlining the walls found, we used the line they formed from edge detection on each side and <ins> calculated the middle point between the two to create a path for the robot to follow </ins>
```python
for i in range(height):
        left_avg = left_edges[i]
        right_avg = right_edges[i]

        if left_avg is not None and right_avg is not None:
            # Calculate the midpoint and draw the path
            middle_line = (left_avg + right_avg) // 2

            # Draw the path line (white) with increased thickness
            cv2.line(edges_black, (middle_line, i), (middle_line, i), (255, 255, 255), 3)  # Increased thickness
```

[You can see the output we got here](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Videos/MIDDLE_LANE_VIDEO_COMPRESSED.mp4)

### Output image
![image](https://github.com/user-attachments/assets/417dd036-7add-4786-abe6-badace10178b)
### When we moved the inner square walls, <ins>the path to follow (middle lane) moved too</ins>
![image](https://github.com/user-attachments/assets/95e8c7f5-c216-4cda-be64-c463440ad920)

### Why we didn't end up using this code (Middle_Lane_Canny) for the first mission
Although this code worked exceptionally well when the robot was faced with long straight paths surrounded by contrasting walls, the same can't be said for turns.
When we tested it with a large camera connected to a laptop, it was facing the track from a very high up angle so it had a better view and could detect turns well.
But, when we tested multiple different cameras connected to a raspberry pi, including a fisheye camera, none of them has a scope wide enough for the current code to predict sharp turns, so the path ended up either guiding the robot straight into the wall, or, turning the opposite direction than expected.  

This wasnt working for us. The camera couldn't be as high up as it was originally when its placed on the robot -in the track- and it also needed the robot to go relatively slow so it can catch up with the camera feedback. This is fine for the second mission, where obstacle avoidance is a higher priority than speed, but for a quick first mission we had to find a new way to sense when and how much the car needs to turn.

>[!NOTE]
>The fact that this detection process didn't work for us doesn't render it unuseable. Combined with the use of a different logic specifically for turns, there is potential for a useable output but we decided to take a different approach.

### Recognising turns - Detecting Blue and Orange lines
Using the same colour picking process we created earlier, we can also detect blue and orange. Using these colours we were able to turn as well as the direction of said turn.

Example values:
## $${\color{blue}Blue}$$
```python
    blue_lower = np.array([100, 60, 80])
    blue_upper = np.array([120, 120, 170])
    mask_blue = cv2.inRange(imgHSV, blue_lower, blue_upper)
```
## $${\color{orange}Orange}$$
```python
    orange_lower = np.array([0, 110, 110])
    orange_upper = np.array([20, 160, 230])
    mask_orange = cv2.inRange(imgHSV, orange_lower, orange_upper)
```

Of course, these values have to be calibrated before the launch of every mission due to the changes in lighting conditions, but we found that this process was much more failproof so it was utilised for the second mission.

The logic of the [Blue and Orange turn detection](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Code%20files/Python%20files/New%20files%20September%2B/Stage%202/Orange%20Blue%20detection.py) Program is as follows:

Using the HSV mask, this time with blue and orange instead of red and green, we detect the presense of those colours in the feed.
The difference between the detection comes with the shape of what we're trying to detect. Instead of the cube shaped obstacles that were being previously detected as squares, we needed to now detect lines - continuous strings of pixels with either colour.

 We detected the lines using Hough Transform
 ```python
    lines_orange = cv2.HoughLinesP(edges_orange, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=10)
    lines_blue = cv2.HoughLinesP(edges_blue, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=10)

    img_lines = img.copy()

    # Draw lines for visualization
    if lines_orange is not None:
        for x1, y1, x2, y2 in lines_orange[:, 0]:
            cv2.line(img_lines, (x1, y1), (x2, y2), (255, 0, 0), 2)

    if lines_blue is not None:
        for x1, y1, x2, y2 in lines_blue[:, 0]:
            cv2.line(img_lines, (x1, y1), (x2, y2), (0, 165, 255), 2)
 ```
We needed to track which of the two lines (If they were visible) was closest to the robot.
Since the camera output is in 2d, the **depth** that we see is changed into **height**.

<p align="center" width="100%">
    From our angle vs from the robots point of view: 
</p>
<p align="center" width="100%">
    <img width="45%" src="https://github.com/user-attachments/assets/3061790a-a697-474f-b5d3-856471fcd83a"> 
    <img width="30%" src="https://github.com/user-attachments/assets/21ff6a7e-6255-4585-8d0d-ccff888c40de"> 
</p>

<p align="center" width="100%">
    From the robot's perpective, the line that is closer to it is also <ins> the line that appears lower on its y axis. </ins>
    <img width="60%" src="https://github.com/user-attachments/assets/7c7c9697-de80-4b93-b7c0-9bc996c9000f"> 
</p>


As you can see in the above image, in this specific scenario where the orange line is clearly closer than the blue one, it is also lower at the point where it intersects with the y axis.
We utilised this property in our code, but instead of using the point y=0 (midpoint) as reference, we used the pixels of our camera output.
Since when we start out camera configuration, we always set it to:

```python
picam2 = Picamera2()
picam2.configure(picam2.create_preview_configuration(main={"size": (640, 480)}))
picam2.start()  
```
So, a total of 640 pixels on the y axis and 480 on the x axis,
>[!NOTE]
>The values start at the top-left most corner where (x,y) is (0,0), Therefore when an object is detected at (example) (x,600), it is at the bottom of the screen.
>Since we want the object to be close to the robot when it turns to avoid turning too early, we set a reference point of y=400 to act as a "secondary x axis", where if orange/blue is detected above that limit (so, on the lower third of the screen), the car should turn.

For safety, we also implemented a condition to eliminate the chanse of both a orange and blue line simulatiously being found bellow the threshold, where the robot only takes into account the one which is closest to itself.

```python
        orange_y = 0
        blue_y = 0
        orange_y = y_at_center(lines_orange)
        blue_y = y_at_center(lines_blue)
 ```

*The values of orange_y and blue_y are originally set to 0 to avoid an error we encountered where the lower line was detected bellow the threshold before the further one was detected at all, returning a non valid value ("NONE") and ending the program abruptly.

```python
        if orange_y > blue_y and orange_y > 400:
            print("Turn right (orange line lower at center)")
            Turn = "Right"
            TURN_ANGLE = 120
        elif blue_y > orange_y and blue_y > 400:
            print("Turn left (blue line lower at center)")
            Turn = "Left"
            TURN_ANGLE = 60
        else:
            print("No line detected at center")
            Turn = "No"
```

Using the above logic combined with an IMU for turning, we managed to make our [imucameraturn.py](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Code%20files/Python%20files/New%20files%20September%2B/Stage%202/imucameraturn.py) program.
> ## A program that:
> Moves forward until either a blue or orange line is detected
> Depending on which one is a) lower and b) bellow our set threshold of 400 pixels, output the turn direction (left or right)
> Steer untill the IMU detects the 90 degree change, by setting the servo angle to point left or right accordingly.

This straight-forward logic was ready to be implemented in the trials for the **second mission.**

# First mission - Final programm description
<a id="top"></a>

<!-- TOC -->
- [Project Overview](#project-overview)
- [Core Functionality](#core-functionality)
- [Control Logic (high level)](#control-logic-high-level)
- [Code Structure](#code-structure)
- [Hardware Setup](#hardware-setup)
- [Configuration](#configuration)
- [System Architecture Diagram](#system-architecture-diagram)
- [Mission Path Diagram](#mission-path-diagram)
<!-- /TOC -->

The program runs on a **Raspberry Pi** and offers two operation modes:
- **GUI Mode (Debug):** Visual interface with real-time plots and tuning sliders.
- **Headless Mode (Competition):** Fully autonomous execution via physical start button.

---

## Core Functionality <a id="core-functionality"></a>
- **Autonomous Navigation:**
  - Follows walls using ToF (VL53L0X) or Ultrasonic sensors.
  - Performs left/right turns based on open-space detection and front thresholds.

- **Finite-State Machine Logic:**
  - Operates under these main states:  
    `IDLE → CRUISE → TURN_INIT → TURNING → POST_TURN → STOPPED`
  - Each state manages specific motor and servo behaviors.

- **Sensor Fusion & Filtering:**
  - Combines ToF/ultrasonic inputs using median, spike rejection, and optional EMA smoothing.
  - Stabilizes readings and eliminates transient noise.

- **Yaw & Turn Control:**
  - Integrates MPU6050 gyro data for angular tracking.
  - Executes precision turns with direction locking, timeouts, and slew-limited servo control.

- **Adaptive Environment Handling:**
  - Detects narrow corridors via side-sum analysis.
  - Dynamically scales speed and correction margins for safer movement.

- **Safety & Lap Control:**
  - Instant front-stop and obstacle recheck timer.
  - Lap counting with automatic halt after the final lap.

---

## Control Logic (high level) <a id="control-logic-high-level"></a>
- **CRUISE:** Straight motion; after 1st turn, apply timed wall-following corrections if side distance < soft margin.
- **TURN_INIT:** When `front < FRONT_TURN_TRIGGER` and lockout OK; keep straight (or gentle correction after 1st turn) while waiting for **exactly one** open side. Apply optional **direction lock** after the first decision.
- **TURNING:** Set servo to the commanded (left/right) angle; integrate gyro Z and **stop when target yaw** is reached within tolerance. Guards: timeout and max yaw.
- **POST_TURN:** Short straight stretch to stabilize, then return to CRUISE.
- **STOPPED:** Triggered by obstacle or max-laps; obstacle mode self‑retries after a wait window.

**Narrow Mode:** If `left+right` (valid readings) < threshold, scale speeds/thresholds to be conservative in tight corridors.

---

## Code Structure <a id="code-structure"></a>

1. Imports
    - External Libraries
    - Internal Modules
2. Variables
    - Global Constants
    - Robot-Specific Variables
3. H/W Setup
    - Hardware Configuration
    - Pin Assignments
4. Robot Classes
    - Sensor Handling
    - Motor Control
    - State Management
5. Robot Loop
    - Main Control Loop
    - Decision Making
    - Movement Logic
6. GUI
    - Tkinter Setup
    - Sliders & Data Visualization
7. Main
    - Initialization
    - Robot Execution
      
---

## Hardware Setup <a id="hardware-setup"></a>
| Component | Function |
|------------|-----------|
| Raspberry Pi | Core computing unit |
| PCA9685 | Motor and servo PWM driver |
| VL53L0X ToF Sensors | Distance measurement |
| Ultrasonic Sensors | Alternative wall sensing |
| MPU6050 | Gyro for yaw tracking |
| LEDs | Status indication |
| Button | Start trigger in headless mode |

---

## Configuration <a id="configuration"></a>
Configuration values are defined in code but can be overridden using `1st_mission_variables.json`.
All GUI slider changes can be saved and reloaded.

---

## System Architecture Diagram <a id="system-architecture-diagram"></a>
```text
 ┌──────────────────────────────────────────┐
 │             SENSOR THREAD                │
 │  - ToF & Ultrasonic readings             │
 │  - Median & EMA filtering                │
 │  - Updates global sensor_data            │
 └──────────────────────────────────────────┘
                    │
                    ▼
 ┌──────────────────────────────────────────┐
 │            ROBOT LOOP (FSM)              │
 │  IDLE → CRUISE → TURN_INIT → TURNING →   │
 │  POST_TURN → STOPPED                     │
 │  - Yaw integration (MPU6050)             │
 │  - Servo & motor control                 │
 │  - Lap counting & safety                 │
 └──────────────────────────────────────────┘
                    │
                    ▼
 ┌──────────────────────────────────────────┐
 │             GUI / HEADLESS MODE          │
 │  - Dashboard                             │
 │  - Sliders, plots, and CSV export        │
 │  - Physical start button (headless)      │
 └──────────────────────────────────────────┘
```

---

## Run Modes <a id="run-modes"></a>
- **Debugging (GUI) Mode:**
  Set `USE_GUI = 1` and press Start Readings & Start Loop button.
- **Competition (Headless) Mode:**
  Set `USE_GUI = 0` and press the hardware start button.

---

# BOM Bill of materials all prices for our robot compartments

## Core System
| Material                         | Amount | Price  |
|----------------------------------|--------|--------|
| Raspberry Pi 5 8GB               | 1      | 96.50 |
| Raspberry Pi Micro SD 64GB       | 1      | 9.50  |
| Active Cooler for Raspberry Pi 5 | 1      | 6.40  |
| GPIO Ribbon Cable 40P            | 1      | 2.00  |
| IDC Connector 2x20 Pin Male      | 1      | 0.45  |

## Sensors
| Material                                      | Amount | Price |
|-----------------------------------------------|--------|-------|
| Camera Arducam IMX219 fisheye lens            | 1      | 18.92 |
| Ultrasonic Sensor HC-SR04                     | 3      | 5.40  |
| Laser Sensor - VL53L0X                        | 6      | 28.80 |
| Triple Axis Gyroscope & Accelerometer MPU6050 | 1      | 3.60  |

## Power & Protection
| Material                                | Amount | Price |
|-----------------------------------------|--------|-------|
| Battery Holder 1x18650 for PCB          | 3      | 2.70  |
| Battery Lithium 18650 3.6V 3400mAh      | 3      | 17.40 |
| Li-ion Battery Charger Protection Board | 1      | 3.40  |
| Fuse 4A                                 | 1      | 0.71  |
| Main Power Switch                       | 1      | 2.23  |
| DC Power Jack 5.5×2.1mm                 | 1      | 0.20  |
| Step-Down Converter LM2596S             | 1      | 2.90  |

## Motors & Motion
| Material                                | Amount | Price |
|-----------------------------------------|--------|-------|
| Metal Gearmotor 25mm – 210RPM 9–12V     | 1      | 6.80  |
| DC Motor Driver Board DRV8871           | 1      | 10.80 |
| Standard Servo MG996R                   | 1      | 8.90  |
| 16-Channel PWM/Servo Driver PCA9685     | 1      | 6.80  |

## Wiring & Connectors
| Material                           | Amount | Price |
|------------------------------------|--------|-------|
| Jumper Wires 30cm 5-Pin            | 6      | 2.50  |
| Molex KF2510 Jumper Wire 4-Pin     | 3      | 1.20  |
| Flexible Flat Cable 0.5mm 22-Pin   | 1      | 2.90  |

## PCB & Prototyping
| Material        | Amount | Price |
|-----------------|--------|-------|
| PCB Prototype   | 5      | 26.25 |
| Tact Switch SMD | 1      | 0.08  |
| LED 0805 SMD    | 3      | 0.36  |
| Resistor 0805   | 12     | 0.24  |

## Fasteners & Hardware
| Material               | Amount | Price |
|------------------------|--------|-------|
| M2 Plastic F/F L20mm   | 2      | 0.30  |
| M2×8mm                 | 8      | 0.47  |
| M3×12mm Flat Head      | 2      | 0.11  |
| M3×16mm Flat Head      | 20     | 1.15  |
| M3×20mm Flat Head      | 14     | 0.97  |
| M3×30mm Flat Head      | 2      | 0.15  |
| M3×50mm                | 2      | 0.70  |

##  [Complete list of all materials amounts and prices](https://github.com/VIVA-LA-VIDA-Hellas/VIVA-LA-VIDA/blob/main/Instruction%20Manuals/Materials%20needed%20and%20BOM.md)
